{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10768305,"sourceType":"datasetVersion","datasetId":6680064},{"sourceId":365476,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":303137,"modelId":323653}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import shutil, os, sys\n\n# Only remove & clone if needed\nif os.path.exists('./pix2pixHD'):\n    shutil.rmtree('./pix2pixHD')\n!git clone https://github.com/NVIDIA/pix2pixHD.git\n\nsys.path.append('./pix2pixHD')\nfrom models.networks import define_G, define_D\n\nprint(\"Repo cloned and imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T09:44:04.465700Z","iopub.execute_input":"2025-05-01T09:44:04.466392Z","iopub.status.idle":"2025-05-01T09:44:09.823805Z","shell.execute_reply.started":"2025-05-01T09:44:04.466345Z","shell.execute_reply":"2025-05-01T09:44:09.822963Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'pix2pixHD'...\nremote: Enumerating objects: 343, done.\u001b[K\nremote: Counting objects: 100% (3/3), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 343 (delta 0), reused 0 (delta 0), pack-reused 340 (from 1)\u001b[K\nReceiving objects: 100% (343/343), 55.68 MiB | 48.90 MiB/s, done.\nResolving deltas: 100% (156/156), done.\nRepo cloned and imports successful!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pix2PixHD + EMA + FP16 (amp/GradScaler)\nimport os, sys, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n!pip install --quiet piq\n!pip install --quiet pytorch-fid\n\nimport csv\nimport os\nimport torch\nimport piq   # for PSNR/SSIM\nfrom pytorch_fid import fid_score\n\ndef calculate_metrics(G, val_loader, device, epoch):\n    G.eval()\n    mse_loss = torch.nn.MSELoss()\n    psnr_list, ssim_list, mse_list = [], [], []\n    pred_dir = '/kaggle/working/fid_pred'\n    gt_dir = '/kaggle/working/fid_gt'\n    os.makedirs(pred_dir, exist_ok=True)\n    os.makedirs(gt_dir, exist_ok=True)\n\n    with torch.no_grad():\n        for i, (cat_input, target_img, _) in enumerate(val_loader):\n            cat_input = cat_input.to(device)\n            target_img = target_img.to(device)\n            fake_img = G(cat_input)\n            fake_img = fake_img.clamp(0., 1.)\n            target_img = target_img.clamp(0., 1.)\n            # Save FID images (first 10 only, for speed)\n            for j in range(min(fake_img.size(0), 3)):\n                pred = fake_img[j].clamp(0,1).cpu().numpy().transpose(1,2,0) * 255\n                pred = pred.astype('uint8')\n                gt = target_img[j].clamp(0,1).cpu().numpy().transpose(1,2,0) * 255\n                gt = gt.astype('uint8')\n                from PIL import Image\n                Image.fromarray(pred).save(os.path.join(pred_dir, f\"{i}_{j}.png\"))\n                Image.fromarray(gt).save(os.path.join(gt_dir, f\"{i}_{j}.png\"))\n            # PSNR, SSIM for first batch only\n            # psnr_val = piq.psnr(fake_img, target_img).item()\n            psnr_val = piq.psnr(fake_img, target_img, data_range=1.).item()\n            # ssim_val = piq.ssim(fake_img, target_img).item()\n            ssim_val = piq.ssim(fake_img, target_img, data_range=1.).item()\n            psnr_list.append(psnr_val)\n            ssim_list.append(ssim_val)\n            mse = mse_loss(fake_img, target_img).item()\n            mse_list.append(mse)\n            break  # Only a single (or few) batches for speed—modify if needed.\n\n    # FID score between pred/gt directories\n    fid = fid_score.calculate_fid_given_paths([gt_dir, pred_dir],\n                                              batch_size=32,\n                                              device=device,\n                                              dims=2048)\n    psnr_avg = sum(psnr_list)/len(psnr_list)\n    ssim_avg = sum(ssim_list)/len(ssim_list)\n    mse_avg = sum(mse_list)/len(mse_list)\n    return psnr_avg, ssim_avg, fid, mse_avg\n\ndef save_metrics_csv(epoch, g_loss, d_loss, psnr, ssim, fid, mse, step, file_path=\"/kaggle/working/metrics.csv\"):\n    file_exists = os.path.isfile(file_path)\n    with open(file_path, \"a\", newline='') as csvfile:\n        fieldnames = [\"epoch\", \"step\", \"g_loss\", \"d_loss\", \"psnr\", \"ssim\", \"fid\", \"mse\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if not file_exists:\n            writer.writeheader()\n        writer.writerow({\n            \"epoch\": epoch,\n            \"step\": step,\n            \"g_loss\": g_loss,\n            \"d_loss\": d_loss,\n            \"psnr\": psnr,\n            \"ssim\": ssim,\n            \"fid\": fid,\n            \"mse\": mse\n        })\n\n\nimport torch\nimport os\n\ndef save_checkpoint(\n    epoch, \n    G, D, ema_g, \n    optimizer_G, optimizer_D, \n    scaler_g, scaler_d, \n    checkpoint_dir='/kaggle/working/checkpoints', \n    extra_dict={}\n):\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    checkpoint = {\n        'epoch': epoch + 1,  # next epoch to continue from\n        # Main weights\n        'generator_state_dict': G.state_dict(),\n        'discriminator_state_dict': D.state_dict(),\n        # Exponential moving average weights\n        'ema_generator_state_dict': ema_g.ema_model.state_dict(),\n        # Optimizers\n        'optimizer_G_state_dict': optimizer_G.state_dict(),\n        'optimizer_D_state_dict': optimizer_D.state_dict(),\n        # For AMP/mixed precision\n        'scaler_g_state_dict': scaler_g.state_dict(),\n        'scaler_d_state_dict': scaler_d.state_dict(),\n    }\n    # checkpoint.update(extra_dict)\n    torch.save(checkpoint, os.path.join(checkpoint_dir, f'pix2pixhd_checkpoint_epoch_{epoch+1}.pth'))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T09:44:09.825326Z","iopub.execute_input":"2025-05-01T09:44:09.825657Z","iopub.status.idle":"2025-05-01T09:44:16.015297Z","shell.execute_reply.started":"2025-05-01T09:44:09.825639Z","shell.execute_reply":"2025-05-01T09:44:16.014468Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =================== Data =====================\nclass LightingDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform\n        self.image_pairs = self.list_image_pairs()\n    def list_image_pairs(self):\n        image_pairs = []\n        scene_folders = [ f for f in os.listdir(self.data_path) if os.path.isdir(os.path.join(self.data_path, f))][:100] \n        for scene_folder in scene_folders:\n            probe_path = os.path.join(self.data_path, scene_folder)\n            if not os.path.isdir(probe_path): continue\n            source_img_path = os.path.join(probe_path, \"dir_0_mip2.jpg\")\n            if not os.path.exists(source_img_path): continue\n            for lighting_idx in range(1, 25):\n                target_img_path = os.path.join(probe_path, f\"dir_{lighting_idx}_mip2.jpg\")\n                if os.path.exists(target_img_path):\n                    image_pairs.append((source_img_path, target_img_path, lighting_idx))\n        return image_pairs\n        \n    def read_image(self, filepath):\n        try:\n            img = Image.open(filepath).convert(\"RGB\")\n            if self.transform: img = self.transform(img)\n            return img\n        except Exception as e:\n            print(f\"Failed to open {filepath}: {e}\")\n            raise\n        \n    def one_hot(self, idx, n=25):\n        onehot = torch.zeros(n, 1, 1)\n        onehot[idx] = 1\n        onehot = onehot.expand(n, 256, 256)\n        return onehot\n    def __len__(self): return len(self.image_pairs)\n    def __getitem__(self, idx):\n        input_img_path, target_img_path, lighting_idx = self.image_pairs[idx]\n        input_img = self.read_image(input_img_path)\n        target_img = self.read_image(target_img_path)\n        lighting_vec = self.one_hot(lighting_idx)\n        cat_input = torch.cat([input_img, lighting_vec], dim=0)\n        return cat_input, target_img, lighting_idx\n\ntransform = transforms.Compose([\n    transforms.Resize((256,256)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor()\n])\n\ndata_path = '/kaggle/input/multi-illumination-jpg/'\ndataset = LightingDataset(data_path, transform=transform)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4, drop_last=True)\n\n    \nbatch = next(iter(dataloader))\nprint(\"Batch shapes:\", [x.shape if hasattr(x, 'shape') else type(x) for x in batch])\n\nprint(\"Total pairs found:\", len(dataset))\nif len(dataset) > 0:\n    sample = dataset[0]\n    print(type(sample), [t.shape if hasattr(t, 'shape') else None for t in sample])\n\n# =================== Pix2PixHD code =====================\nfrom models.networks import define_G, define_D\n\ninput_nc = 28\noutput_nc = 3\n\n\n\n\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nG = define_G(input_nc, output_nc, 64, 'global', 4, 9, 1, 3, 'instance', [])\nprint(\"G defined\")\nD = define_D(input_nc + output_nc, 64, 3, 'instance', False, 2, False, [])\nprint(\"D defined\")\n\n\nprint(\"Both initializations complete\")\n\nG = nn.DataParallel(G).to(device)\nD = nn.DataParallel(D).to(device)\n\ncriterionGAN = nn.MSELoss()\ncriterionL1 = nn.L1Loss()\nlr, beta1 = 0.0002, 0.5\noptimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\n\n\nprint(\"EMA starts\")\n# =================== EMA ======================\nimport copy\nclass EMA:\n    def __init__(self, model, decay=0.999):\n        self.model = model\n        self.ema_model = copy.deepcopy(model)\n        self.decay = decay\n        for p in self.ema_model.parameters():\n            p.requires_grad = False\n        self.ema_model.eval()\n    def update(self):\n        with torch.no_grad():\n            for ema_param, param in zip(self.ema_model.parameters(), self.model.parameters()):\n                ema_param.data.mul_(self.decay).add_((1. - self.decay) * param.data)\n    def to(self, device):\n        self.ema_model.to(device)\n        return self\n    def module(self):\n        return self.ema_model\n\nema_g = EMA(G, decay=0.999).to(device)\n\nprint(\"EMA complete\")\n# =================== FP16/AMP =====================\nscaler_g = torch.amp.GradScaler('cuda')\nscaler_d = torch.amp.GradScaler('cuda')\n\n# =================== Training Loop =================\n\nprint(\"Scalers defined\")\n\ndef denorm(x):\n    arr = x.detach().cpu().numpy()\n    if arr.dtype != np.float32 and arr.dtype != np.float64:\n        arr = arr.astype(np.float32)                # Force to float32\n    arr = np.clip(arr, 0, 1)\n    arr = (arr * 255).astype(np.uint8)              # Now uint8 for imshow\n    if arr.shape[0] == 1:                           # handle grayscale\n        arr = arr[0]\n    elif arr.shape[0] == 3:                         # CHW -> HWC\n        arr = np.transpose(arr, (1,2,0))\n    return arr\n\n\nepochs, lambda_L1 = 30, 100\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T09:44:16.016411Z","iopub.execute_input":"2025-05-01T09:44:16.017310Z","iopub.status.idle":"2025-05-01T09:44:23.028643Z","shell.execute_reply.started":"2025-05-01T09:44:16.017283Z","shell.execute_reply":"2025-05-01T09:44:23.027729Z"}},"outputs":[{"name":"stdout","text":"Batch shapes: [torch.Size([8, 28, 256, 256]), torch.Size([8, 3, 256, 256]), torch.Size([8])]\nTotal pairs found: 2400\n<class 'tuple'> [torch.Size([28, 256, 256]), torch.Size([3, 256, 256]), None]\nGlobalGenerator(\n  (model): Sequential(\n    (0): ReflectionPad2d((3, 3, 3, 3))\n    (1): Conv2d(28, 64, kernel_size=(7, 7), stride=(1, 1))\n    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (3): ReLU(inplace=True)\n    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (12): ReLU(inplace=True)\n    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (15): ReLU(inplace=True)\n    (16): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (17): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (18): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (19): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (20): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (21): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (22): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (23): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (24): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (27): ReLU(inplace=True)\n    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (30): ReLU(inplace=True)\n    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (33): ReLU(inplace=True)\n    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (36): ReLU(inplace=True)\n    (37): ReflectionPad2d((3, 3, 3, 3))\n    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n    (39): Tanh()\n  )\n)\nG defined\nMultiscaleDiscriminator(\n  (layer0): Sequential(\n    (0): Conv2d(31, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n  )\n  (layer1): Sequential(\n    (0): Conv2d(31, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n  )\n  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n)\nD defined\nBoth initializations complete\nEMA starts\nEMA complete\nScalers defined\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/input/pix2pix-hd-lighting/pytorch/epoch30/6/pix2pixhd_checkpoint_epoch_60.pth', map_location=device)\nstart_epoch = checkpoint['epoch']\nG.load_state_dict(checkpoint['generator_state_dict'])\nD.load_state_dict(checkpoint['discriminator_state_dict'])\nema_g.ema_model.load_state_dict(checkpoint['ema_generator_state_dict'])\n\noptimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\noptimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n\nscaler_g.load_state_dict(checkpoint['scaler_g_state_dict'])\nscaler_d.load_state_dict(checkpoint['scaler_d_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T09:44:23.030562Z","iopub.execute_input":"2025-05-01T09:44:23.030808Z","iopub.status.idle":"2025-05-01T09:44:25.936612Z","shell.execute_reply.started":"2025-05-01T09:44:23.030789Z","shell.execute_reply":"2025-05-01T09:44:25.936057Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_173/4155050622.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/input/pix2pix-hd-lighting/pytorch/epoch30/6/pix2pixhd_checkpoint_epoch_60.pth', map_location=device)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"Epoch starts\")\n\nfor epoch in range(epochs):\n    G.train(); D.train()\n    for i, (cat_input, target_img, _) in enumerate(dataloader):\n        cat_input = cat_input.to(device)\n        target_img = target_img.to(device)\n        #========== Train G (autocast/FP16) ===========\n        optimizer_G.zero_grad()\n        with torch.amp.autocast('cuda'):\n            fake_out = G(cat_input)\n            fake_pair = torch.cat([cat_input, fake_out], 1)\n            pred_fake = D(fake_pair)\n            loss_G_GAN = 0\n            for pred in pred_fake:\n                if isinstance(pred, list):\n                    pred = pred[-1]\n                loss_G_GAN += criterionGAN(pred, torch.ones_like(pred))\n            loss_G_GAN /= len(pred_fake)\n            loss_G_L1 = criterionL1(fake_out, target_img) * lambda_L1\n            loss_G = loss_G_GAN + loss_G_L1\n        scaler_g.scale(loss_G).backward()\n        scaler_g.step(optimizer_G)\n        scaler_g.update()\n\n        ema_g.update()\n        #========== Train D (autocast/FP16) ===========\n        optimizer_D.zero_grad()\n        with torch.amp.autocast('cuda'):\n            real_pair = torch.cat([cat_input, target_img], 1)\n            pred_real = D(real_pair)\n            \n            loss_D_real = 0\n            for pr in pred_real:\n                if isinstance(pr, list):\n                    pr = pr[-1]\n                loss_D_real += criterionGAN(pr, torch.ones_like(pr))\n            loss_D_real /= len(pred_real)\n            pred_fake_detach = D(fake_pair.detach())\n            \n            loss_D_fake = 0\n            for pf in pred_fake_detach:\n                if isinstance(pf, list):\n                    pf = pf[-1]\n                loss_D_fake += criterionGAN(pf, torch.ones_like(pf))\n            loss_D_fake /= len(pred_fake_detach)\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n        scaler_d.scale(loss_D).backward()\n        scaler_d.step(optimizer_D)\n        scaler_d.update()\n        if i % 100 == 0:\n            print(f\"Epoch [{start_epoch+epoch+1}/{start_epoch+epochs}] Batch [{i}] | LossG: {loss_G.item():.4f} | LossD: {loss_D.item():.4f}\")\n            # Assume dataloader is your train loader; for FID, use a separate validation or subset if possible\n            psnr, ssim, fid, mse = calculate_metrics(ema_g.ema_model, dataloader, device, start_epoch+epoch+1)\n            save_metrics_csv(start_epoch+epoch+1, loss_G.item(), loss_D.item(), psnr, ssim, fid, mse, i % 100)\n\n    #========= Visualization (use EMA generator) =========\n    import os\n\n    G.eval(); ema_g.ema_model.eval()\n    os.makedirs('/kaggle/working/visualizations', exist_ok=True)\n    with torch.no_grad(), torch.amp.autocast('cuda'):\n        val_input, val_target, _ = next(iter(dataloader))\n        val_input = val_input.to(device)\n        val_target = val_target.to(device)\n        fake_img = ema_g.ema_model(val_input)\n    \n        # Take first sample in batch\n        inp = denorm(val_input[0, :3])\n        tgt = denorm(val_target[0])\n        pred = denorm(fake_img[0])\n    \n        import matplotlib.pyplot as plt\n        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n        axs[0].imshow(denorm(val_input[0, :3]))\n        axs[0].set_title(\"Input Image\")\n        axs[1].imshow(denorm(val_target[0]))\n        axs[1].set_title(\"Target Lighting GT\")\n        axs[2].imshow(denorm(fake_img[0]))\n        axs[2].set_title(\"Predicted Lighting (EMA G)\")\n\n\n        for a in axs: a.axis('off')\n        output_path = f\"/kaggle/working/visualizations/epoch_{start_epoch+epoch+1:02d}_viz.png\"\n        plt.savefig(output_path, bbox_inches='tight')\n        plt.close(fig)\n    \n        # Optionally, also save the images individually if preferred:\n        from PIL import Image\n        Image.fromarray((inp * 255).astype(np.uint8)).save(f\"/kaggle/working/visualizations/epoch_{start_epoch+epoch+1:02d}_input.png\")\n        Image.fromarray((tgt * 255).astype(np.uint8)).save(f\"/kaggle/working/visualizations/epoch_{start_epoch+epoch+1:02d}_gt.png\")\n        Image.fromarray((pred * 255).astype(np.uint8)).save(f\"/kaggle/working/visualizations/epoch_{start_epoch+epoch+1:02d}_pred.png\")\n\n# save_checkpoint(start_epoch+epochs, G, D, ema_g, optimizer_G, optimizer_D, scaler_g, scaler_d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T09:44:25.937332Z","iopub.execute_input":"2025-05-01T09:44:25.937602Z","iopub.status.idle":"2025-05-01T11:13:58.696009Z","shell.execute_reply.started":"2025-05-01T09:44:25.937583Z","shell.execute_reply":"2025-05-01T11:13:58.694993Z"}},"outputs":[{"name":"stdout","text":"Epoch starts\nEpoch [61/90] Batch [0] | LossG: 10.5024 | LossD: 0.0006\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n100%|██████████| 91.2M/91.2M [00:00<00:00, 127MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [61/90] Batch [100] | LossG: 10.7174 | LossD: 0.0018\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [61/90] Batch [200] | LossG: 11.6048 | LossD: 0.0005\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [62/90] Batch [0] | LossG: 8.5878 | LossD: 0.0008\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [62/90] Batch [100] | LossG: 9.7755 | LossD: 0.0023\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [62/90] Batch [200] | LossG: 8.1203 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [63/90] Batch [0] | LossG: 10.7820 | LossD: 0.0008\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [63/90] Batch [100] | LossG: 10.6848 | LossD: 0.0012\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [63/90] Batch [200] | LossG: 10.6207 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [64/90] Batch [0] | LossG: 14.7170 | LossD: 0.0032\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  7.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [64/90] Batch [100] | LossG: 11.4855 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [64/90] Batch [200] | LossG: 7.8446 | LossD: 0.0010\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [65/90] Batch [0] | LossG: 12.6195 | LossD: 0.0019\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [65/90] Batch [100] | LossG: 8.3376 | LossD: 0.0015\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [65/90] Batch [200] | LossG: 15.3534 | LossD: 0.0015\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [66/90] Batch [0] | LossG: 12.3447 | LossD: 0.0008\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [66/90] Batch [100] | LossG: 10.9028 | LossD: 0.0020\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [66/90] Batch [200] | LossG: 8.2832 | LossD: 0.0015\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [67/90] Batch [0] | LossG: 7.0323 | LossD: 0.0024\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [67/90] Batch [100] | LossG: 14.3666 | LossD: 0.0011\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [67/90] Batch [200] | LossG: 14.4106 | LossD: 0.0006\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [68/90] Batch [0] | LossG: 13.2963 | LossD: 0.0042\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [68/90] Batch [100] | LossG: 10.2468 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [68/90] Batch [200] | LossG: 11.7740 | LossD: 0.0004\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [69/90] Batch [0] | LossG: 12.8868 | LossD: 0.0017\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [69/90] Batch [100] | LossG: 9.3391 | LossD: 0.0002\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [69/90] Batch [200] | LossG: 13.9560 | LossD: 0.0001\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [70/90] Batch [0] | LossG: 8.9516 | LossD: 0.0004\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [70/90] Batch [100] | LossG: 9.7954 | LossD: 0.0013\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [70/90] Batch [200] | LossG: 13.0166 | LossD: 0.0037\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [71/90] Batch [0] | LossG: 10.8316 | LossD: 0.0005\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [71/90] Batch [100] | LossG: 13.1223 | LossD: 0.0025\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [71/90] Batch [200] | LossG: 10.1105 | LossD: 0.0001\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [72/90] Batch [0] | LossG: 11.3466 | LossD: 0.0000\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [72/90] Batch [100] | LossG: 11.1357 | LossD: 0.0001\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [72/90] Batch [200] | LossG: 13.0202 | LossD: 0.0001\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [73/90] Batch [0] | LossG: 5.9009 | LossD: 0.0008\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [73/90] Batch [100] | LossG: 13.2584 | LossD: 0.0003\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [73/90] Batch [200] | LossG: 7.3984 | LossD: 0.0005\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [74/90] Batch [0] | LossG: 8.4028 | LossD: 0.0001\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [74/90] Batch [100] | LossG: 9.2887 | LossD: 0.0004\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [74/90] Batch [200] | LossG: 12.7967 | LossD: 0.0010\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [75/90] Batch [0] | LossG: 10.5360 | LossD: 0.0006\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [75/90] Batch [100] | LossG: 8.8216 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [75/90] Batch [200] | LossG: 14.8369 | LossD: 0.0005\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [76/90] Batch [0] | LossG: 12.0696 | LossD: 0.0005\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [76/90] Batch [100] | LossG: 7.7638 | LossD: 0.0017\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [76/90] Batch [200] | LossG: 10.9216 | LossD: 0.0003\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [77/90] Batch [0] | LossG: 13.4808 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [77/90] Batch [100] | LossG: 9.9744 | LossD: 0.0014\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [77/90] Batch [200] | LossG: 10.3152 | LossD: 0.0003\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [78/90] Batch [0] | LossG: 8.9631 | LossD: 0.0001\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [78/90] Batch [100] | LossG: 10.5753 | LossD: 0.0181\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [78/90] Batch [200] | LossG: 10.2856 | LossD: 0.0003\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [79/90] Batch [0] | LossG: 11.8805 | LossD: 0.0005\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [79/90] Batch [100] | LossG: 14.6743 | LossD: 0.0012\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [79/90] Batch [200] | LossG: 10.6452 | LossD: 0.0008\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [80/90] Batch [0] | LossG: 10.9828 | LossD: 0.0004\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [80/90] Batch [100] | LossG: 10.6492 | LossD: 0.0009\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [80/90] Batch [200] | LossG: 6.9537 | LossD: 0.0011\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [81/90] Batch [0] | LossG: 11.3383 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [81/90] Batch [100] | LossG: 7.7830 | LossD: 0.0008\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [81/90] Batch [200] | LossG: 11.7373 | LossD: 0.0010\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [82/90] Batch [0] | LossG: 8.6503 | LossD: 0.0004\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [82/90] Batch [100] | LossG: 13.0216 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [82/90] Batch [200] | LossG: 11.7111 | LossD: 0.0010\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [83/90] Batch [0] | LossG: 11.0620 | LossD: 0.0016\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [83/90] Batch [100] | LossG: 13.3080 | LossD: 0.0019\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [83/90] Batch [200] | LossG: 7.6828 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [84/90] Batch [0] | LossG: 11.2751 | LossD: 0.0004\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [84/90] Batch [100] | LossG: 8.2437 | LossD: 0.0010\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [84/90] Batch [200] | LossG: 10.3661 | LossD: 0.0031\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [85/90] Batch [0] | LossG: 12.7440 | LossD: 0.0005\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [85/90] Batch [100] | LossG: 13.3737 | LossD: 0.0049\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [85/90] Batch [200] | LossG: 10.3173 | LossD: 0.0001\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [86/90] Batch [0] | LossG: 9.6743 | LossD: 0.0002\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [86/90] Batch [100] | LossG: 11.1696 | LossD: 0.0001\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  7.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [86/90] Batch [200] | LossG: 15.3175 | LossD: 0.0000\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [87/90] Batch [0] | LossG: 8.8004 | LossD: 0.0003\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [87/90] Batch [100] | LossG: 9.8021 | LossD: 0.0000\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [87/90] Batch [200] | LossG: 10.7459 | LossD: 0.0000\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [88/90] Batch [0] | LossG: 12.0105 | LossD: 0.0018\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [88/90] Batch [100] | LossG: 14.6024 | LossD: 0.0000\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [88/90] Batch [200] | LossG: 10.4517 | LossD: 0.0004\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [89/90] Batch [0] | LossG: 12.1056 | LossD: 0.0060\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [89/90] Batch [100] | LossG: 10.5975 | LossD: 0.0000\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [89/90] Batch [200] | LossG: 13.7431 | LossD: 0.0000\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  6.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [90/90] Batch [0] | LossG: 9.7675 | LossD: 0.0000\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [90/90] Batch [100] | LossG: 12.6716 | LossD: 0.0007\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [90/90] Batch [200] | LossG: 9.7791 | LossD: 0.0004\nWarning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Warning: batch size is bigger than the data size. Setting batch size to data size\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\n# save_checkpoint(start_epoch+epochs, G, D, ema_g, optimizer_G, optimizer_D, scaler_g, scaler_d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T11:13:58.697211Z","iopub.execute_input":"2025-05-01T11:13:58.697458Z","iopub.status.idle":"2025-05-01T11:13:58.701844Z","shell.execute_reply.started":"2025-05-01T11:13:58.697436Z","shell.execute_reply":"2025-05-01T11:13:58.701012Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# os.makedirs('/kaggle/working/checkpoints/', exist_ok=True)\ncheckpoint = {\n    'epoch': start_epoch+epoch + 1,\n    'generator_state_dict': G.state_dict(),\n    'discriminator_state_dict': D.state_dict(),\n    'ema_generator_state_dict': ema_g.ema_model.state_dict(),\n    'optimizer_G_state_dict': optimizer_G.state_dict(),\n    'optimizer_D_state_dict': optimizer_D.state_dict(),\n    'scaler_g_state_dict': scaler_g.state_dict(),\n    'scaler_d_state_dict': scaler_d.state_dict(),\n}\ntorch.save(checkpoint, os.path.join('/kaggle/working/', f'pix2pixhd_checkpoint_epoch_{start_epoch+epoch+1}.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T11:13:58.702518Z","iopub.execute_input":"2025-05-01T11:13:58.702762Z","iopub.status.idle":"2025-05-01T11:14:04.537216Z","shell.execute_reply.started":"2025-05-01T11:13:58.702745Z","shell.execute_reply":"2025-05-01T11:14:04.536406Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!rm file.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T11:14:04.538258Z","iopub.execute_input":"2025-05-01T11:14:04.538592Z","iopub.status.idle":"2025-05-01T11:14:04.715324Z","shell.execute_reply.started":"2025-05-01T11:14:04.538560Z","shell.execute_reply":"2025-05-01T11:14:04.714417Z"}},"outputs":[{"name":"stdout","text":"rm: cannot remove 'file.zip': No such file or directory\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# import shutil\n# shutil.make_archive('all', 'zip', '/kaggle/working/')\n\n!zip -r file.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T11:14:04.716505Z","iopub.execute_input":"2025-05-01T11:14:04.716741Z","iopub.status.idle":"2025-05-01T11:14:55.910495Z","shell.execute_reply.started":"2025-05-01T11:14:04.716719Z","shell.execute_reply":"2025-05-01T11:14:55.909751Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/pix2pixhd_checkpoint_epoch_60.pth^C\n\n\n\nzip error: Interrupted (aborting)\n","output_type":"stream"}],"execution_count":9}]}