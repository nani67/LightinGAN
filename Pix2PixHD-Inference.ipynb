{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10768305,"sourceType":"datasetVersion","datasetId":6680064},{"sourceId":368443,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":303137,"modelId":323653}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import shutil, os, sys\n\n# Only remove & clone if needed\nif os.path.exists('./pix2pixHD'):\n    shutil.rmtree('./pix2pixHD')\n!git clone https://github.com/NVIDIA/pix2pixHD.git\n\nsys.path.append('./pix2pixHD')\nfrom models.networks import define_G, define_D\n\nprint(\"Repo cloned and imports successful!\")\n\n\nimport torch\nfrom models.networks import define_G\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\n\n# ----------------- Setup and Model Loading -----------------\n# Set your paths\ncheckpoint_path = \"/kaggle/input/pix2pix-hd-lighting/pytorch/epoch30/7/pix2pixhd_checkpoint_epoch_90.pth\"  # update with your best/last epoch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Model parameters (as in your training code)\ninput_nc = 28\noutput_nc = 3\nG = define_G(input_nc, output_nc, 64, 'global', 4, 9, 1, 3, 'instance', [])\nG = torch.nn.DataParallel(G)\nG.to(device)\nG.eval()\n\n# Load EMA generator weights\ncheckpoint = torch.load(checkpoint_path, map_location=device)\nG.load_state_dict(checkpoint[\"ema_generator_state_dict\"], strict=False)  # We use EMA weights for inference\nprint(\"Loaded EMA weights.\")\n\n# ----------------- Helper Functions -----------------\ndef preprocess_input(img_path, lighting_idx):\n    # Image Transform (must match training)\n    transform = transforms.Compose([\n        transforms.Resize((256,256)),\n        transforms.ToTensor()\n    ])\n    img = Image.open(img_path).convert(\"RGB\")\n    img = transform(img)\n    # Create one-hot lighting vector\n    n = 25\n    lighting_vec = torch.zeros(n, 1, 1)\n    lighting_vec[lighting_idx] = 1\n    lighting_vec = lighting_vec.expand(n, 256, 256)\n    # Concatenate on channel axis\n    cat_input = torch.cat([img, lighting_vec], dim=0)  # shape: [28,256,256]\n    return cat_input.unsqueeze(0)  # Add batch dimension\n\ndef save_output(tensor_img, save_path):\n    arr = tensor_img.detach().cpu().clamp(0,1).numpy()[0]  # [3, H, W]\n    arr = np.transpose(arr, (1,2,0))  # [H, W, 3]\n    arr = (arr * 255).astype(np.uint8)\n    Image.fromarray(arr).save(save_path)\n    print(f\"Saved output: {save_path}\")\n\n# ----------------- Inference -----------------\n# Example usage:\ninput_img_path = \"/kaggle/input/multi-illumination-jpg/14n_copyroom1/dir_0_mip2.jpg\"\nlighting_idx = 5  # for example, use target lighting index 5\n\nfor i in range(1,24):\n    cat_input = preprocess_input(input_img_path, i).to(device)\n    with torch.no_grad(), torch.amp.autocast(device.type):\n        output = G(cat_input)\n        output = output.clamp(0., 1.)\n    save_output(output, f\"/kaggle/working/inference_output_{i}.png\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:51:14.898544Z","iopub.execute_input":"2025-05-02T18:51:14.899000Z","iopub.status.idle":"2025-05-02T18:51:44.468271Z","shell.execute_reply.started":"2025-05-02T18:51:14.898974Z","shell.execute_reply":"2025-05-02T18:51:44.467529Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'pix2pixHD'...\nremote: Enumerating objects: 343, done.\u001b[K\nremote: Counting objects: 100% (3/3), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 343 (delta 0), reused 0 (delta 0), pack-reused 340 (from 1)\u001b[K\nReceiving objects: 100% (343/343), 55.68 MiB | 39.16 MiB/s, done.\nResolving deltas: 100% (156/156), done.\nRepo cloned and imports successful!\nGlobalGenerator(\n  (model): Sequential(\n    (0): ReflectionPad2d((3, 3, 3, 3))\n    (1): Conv2d(28, 64, kernel_size=(7, 7), stride=(1, 1))\n    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (3): ReLU(inplace=True)\n    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (12): ReLU(inplace=True)\n    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (15): ReLU(inplace=True)\n    (16): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (17): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (18): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (19): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (20): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (21): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (22): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (23): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (24): ResnetBlock(\n      (conv_block): Sequential(\n        (0): ReflectionPad2d((1, 1, 1, 1))\n        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n        (3): ReLU(inplace=True)\n        (4): ReflectionPad2d((1, 1, 1, 1))\n        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      )\n    )\n    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (27): ReLU(inplace=True)\n    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (30): ReLU(inplace=True)\n    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (33): ReLU(inplace=True)\n    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n    (36): ReLU(inplace=True)\n    (37): ReflectionPad2d((3, 3, 3, 3))\n    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n    (39): Tanh()\n  )\n)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_110/2476682449.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Loaded EMA weights.\nSaved output: /kaggle/working/inference_output_1.png\nSaved output: /kaggle/working/inference_output_2.png\nSaved output: /kaggle/working/inference_output_3.png\nSaved output: /kaggle/working/inference_output_4.png\nSaved output: /kaggle/working/inference_output_5.png\nSaved output: /kaggle/working/inference_output_6.png\nSaved output: /kaggle/working/inference_output_7.png\nSaved output: /kaggle/working/inference_output_8.png\nSaved output: /kaggle/working/inference_output_9.png\nSaved output: /kaggle/working/inference_output_10.png\nSaved output: /kaggle/working/inference_output_11.png\nSaved output: /kaggle/working/inference_output_12.png\nSaved output: /kaggle/working/inference_output_13.png\nSaved output: /kaggle/working/inference_output_14.png\nSaved output: /kaggle/working/inference_output_15.png\nSaved output: /kaggle/working/inference_output_16.png\nSaved output: /kaggle/working/inference_output_17.png\nSaved output: /kaggle/working/inference_output_18.png\nSaved output: /kaggle/working/inference_output_19.png\nSaved output: /kaggle/working/inference_output_20.png\nSaved output: /kaggle/working/inference_output_21.png\nSaved output: /kaggle/working/inference_output_22.png\nSaved output: /kaggle/working/inference_output_23.png\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!zip file.zip /kaggle/working/*.png","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T18:52:22.282000Z","iopub.execute_input":"2025-05-02T18:52:22.282439Z","iopub.status.idle":"2025-05-02T18:52:22.498821Z","shell.execute_reply.started":"2025-05-02T18:52:22.282413Z","shell.execute_reply":"2025-05-02T18:52:22.497404Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/inference_output_10.png (deflated 0%)\n  adding: kaggle/working/inference_output_11.png (deflated 0%)\n  adding: kaggle/working/inference_output_12.png (deflated 0%)\n  adding: kaggle/working/inference_output_13.png (deflated 0%)\n  adding: kaggle/working/inference_output_14.png (deflated 0%)\n  adding: kaggle/working/inference_output_15.png (deflated 0%)\n  adding: kaggle/working/inference_output_16.png (deflated 0%)\n  adding: kaggle/working/inference_output_17.png (deflated 0%)\n  adding: kaggle/working/inference_output_18.png (deflated 0%)\n  adding: kaggle/working/inference_output_19.png (deflated 0%)\n  adding: kaggle/working/inference_output_1.png (deflated 0%)\n  adding: kaggle/working/inference_output_20.png (deflated 0%)\n  adding: kaggle/working/inference_output_21.png (deflated 0%)\n  adding: kaggle/working/inference_output_22.png (deflated 0%)\n  adding: kaggle/working/inference_output_23.png (deflated 0%)\n  adding: kaggle/working/inference_output_2.png (deflated 0%)\n  adding: kaggle/working/inference_output_3.png (deflated 0%)\n  adding: kaggle/working/inference_output_4.png (deflated 0%)\n  adding: kaggle/working/inference_output_5.png (deflated 0%)\n  adding: kaggle/working/inference_output_6.png (deflated 0%)\n  adding: kaggle/working/inference_output_7.png (deflated 0%)\n  adding: kaggle/working/inference_output_8.png (deflated 0%)\n  adding: kaggle/working/inference_output_9.png (deflated 0%)\n","output_type":"stream"}],"execution_count":2}]}